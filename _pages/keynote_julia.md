### [Julia Kreutzer](https://juliakreutzer.github.io/)

**Title:** Optimizing Multilinguality Post Training 

**Abstract:** Is multilinguality determined in pretraining? Starting from insights around building Aya Expanse, this talk will provide a few examples of how we can optimize multilinguality post pre-training, with RL, via test-time scaling, and in data distillation or synthetic data generation. We will learn that techniques optimized for English sometimes disappoint, but that there are surprisingly simple and effective tricks to bootstrap multilingual performance, especially when focusing on open-ended tasks.


**Bio:** Julia Kreutzer is a Senior Research Scientist at Cohere Labs, where she conducts research on multilingual large language models and their evaluation. Previously, she worked on machine translation, in her previous role at Google Translate, and during her PhD at Heidelberg University and in collaborations with grassroots NLP communities for lower-resource languages.